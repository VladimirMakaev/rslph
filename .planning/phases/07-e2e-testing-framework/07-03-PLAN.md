---
phase: 07-e2e-testing-framework
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - fake_claude/scenario.py
  - tests/e2e/test_edge_cases.py
  - tests/e2e/test_multi_invoke.py
autonomous: true

must_haves:
  truths:
    - "Fake Claude can simulate timeout by delaying output"
    - "Fake Claude can simulate crash by exiting mid-stream"
    - "Fake Claude can output malformed JSON for error testing"
    - "Multiple invocations use correct response based on invocation number"
    - "Invocation history is tracked for test assertions"
  artifacts:
    - path: "tests/e2e/test_edge_cases.py"
      provides: "Tests for timeout, crash, malformed output scenarios"
      min_lines: 80
    - path: "tests/e2e/test_multi_invoke.py"
      provides: "Tests for multi-invocation retry/failure memory"
      min_lines: 60
  key_links:
    - from: "tests/e2e/test_edge_cases.py"
      to: "fake_claude/scenario.py"
      via: "Uses crash_after, with_delay, send_raw methods"
      pattern: "(crash_after|with_delay|send_raw)"
    - from: "tests/e2e/test_multi_invoke.py"
      to: "fake_claude/scenario.py"
      via: "Uses next_invocation() for chained responses"
      pattern: "next_invocation"
---

<objective>
Add edge case simulation and multi-invocation support to fake-claude, with comprehensive tests.

Purpose: Enable testing of error handling (timeout, crash, malformed output) and multi-iteration scenarios (retry after failure, accumulated learning).

Output: Enhanced fake_claude API with edge case methods, plus E2E tests exercising these scenarios.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-e2e-testing-framework/07-CONTEXT.md

# Prior plan outputs
@.planning/phases/07-e2e-testing-framework/07-01-SUMMARY.md
@.planning/phases/07-e2e-testing-framework/07-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add edge case methods to InvocationBuilder</name>
  <files>
    fake_claude/scenario.py
    fake_claude/executable.py
  </files>
  <action>
Enhance InvocationBuilder in `fake_claude/scenario.py`:

1. Add edge case methods:
   - `send_raw(raw_text: str) -> InvocationBuilder` - output arbitrary text (for malformed JSON testing)
   - Ensure `crash_after(lines: int)` is fully implemented - exit after N output lines
   - Ensure `with_delay(seconds: float)` works - delay between each output line
   - Add `with_timeout_simulation(seconds: float)` - long delay to trigger caller timeout

2. Add invocation tracking to Scenario:
   - `get_invocation_count() -> int` - how many times executable was called
   - `get_invocation_log() -> list[dict]` - list of invocation metadata (args, time, etc.)

3. Update `fake_claude/executable.py` to:
   - Log each invocation to a temp file (test_id + "_log.json")
   - Include: invocation number, timestamp, sys.argv
   - Handle send_raw by outputting raw text (not JSON)
   - Handle crash_after by sys.exit(1) after N lines

4. Add `Scenario.read_invocation_log()` method that reads the log file.
  </action>
  <verify>
```bash
cd /Users/vmakaev/NonWork/rslph
python3 -c "
from fake_claude import fake_claude
import subprocess

# Test crash_after
s = fake_claude().on_invocation(1).respond_with_text('line1').respond_with_text('line2').crash_after(1).build()
result = subprocess.run([s.executable_path], capture_output=True, text=True)
print('Crash exit code:', result.returncode)
s.cleanup()
"
```
  </verify>
  <done>Edge case methods (send_raw, crash_after, with_delay) work, invocation logging tracks calls</done>
</task>

<task type="auto">
  <name>Task 2: Create edge case tests</name>
  <files>
    tests/e2e/test_edge_cases.py
  </files>
  <action>
Create `tests/e2e/test_edge_cases.py` with tests for error scenarios:

```python
"""Tests for edge case scenarios: timeout, crash, malformed output."""
import subprocess
import time
import json
from fake_claude import fake_claude

def test_crash_mid_stream(workspace, rslph_binary):
    """Fake Claude crashes after partial output."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Starting work...")
        .crash_after(1)  # Exit after first output line
        .build()
    )

    workspace.with_claude_path(scenario.executable_path)
    workspace.write_progress("# Tasks\n- [ ] Task 1")
    workspace.commit_all("Initial")

    # Run rslph - should handle crash gracefully
    result = subprocess.run(
        [str(rslph_binary), "build", "--max-iterations", "1"],
        cwd=workspace.path,
        capture_output=True,
        text=True,
        timeout=30
    )

    # rslph should not crash itself
    # (may exit non-zero due to Claude failure, but shouldn't hang or panic)
    scenario.cleanup()

def test_delayed_output(workspace, rslph_binary):
    """Fake Claude outputs slowly (tests no premature timeout)."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Working...")
        .with_delay(0.5)  # 500ms between lines
        .respond_with_text("Done!")
        .build()
    )

    workspace.with_claude_path(scenario.executable_path)
    workspace.write_progress("# Tasks\n- [ ] Task 1")
    workspace.commit_all("Initial")

    start = time.time()
    result = subprocess.run(
        [str(rslph_binary), "build", "--max-iterations", "1"],
        cwd=workspace.path,
        capture_output=True,
        text=True,
        timeout=30
    )
    elapsed = time.time() - start

    # Should take at least 0.5s due to delay
    assert elapsed >= 0.4, f"Output was too fast: {elapsed}s"
    scenario.cleanup()

def test_malformed_json_output():
    """Fake Claude outputs invalid JSON (raw escape hatch)."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .send_raw("not valid json at all\n")
        .send_raw("{broken: json}\n")
        .build()
    )

    result = subprocess.run(
        [scenario.executable_path],
        capture_output=True,
        text=True
    )

    assert "not valid json" in result.stdout
    assert "{broken: json}" in result.stdout
    scenario.cleanup()

def test_empty_response():
    """Fake Claude returns empty response (unconfigured invocation)."""
    scenario = fake_claude().build()  # No invocations configured

    result = subprocess.run(
        [scenario.executable_path],
        capture_output=True,
        text=True
    )

    # Should exit 0 with empty/minimal output
    assert result.returncode == 0
    scenario.cleanup()

def test_fast_output():
    """Fake Claude outputs very quickly (tests no buffer issues)."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Line 1")
        .respond_with_text("Line 2")
        .respond_with_text("Line 3")
        .respond_with_text("Line 4")
        .respond_with_text("Line 5")
        .with_delay(0)  # Instant
        .build()
    )

    result = subprocess.run(
        [scenario.executable_path],
        capture_output=True,
        text=True
    )

    lines = [l for l in result.stdout.strip().split('\n') if l]
    assert len(lines) >= 5, f"Expected 5+ lines, got {len(lines)}"
    scenario.cleanup()
```
  </action>
  <verify>
```bash
cd /Users/vmakaev/NonWork/rslph
python3 -m pytest tests/e2e/test_edge_cases.py -v --tb=short
```
  </verify>
  <done>Edge case tests pass: crash, delay, malformed JSON, empty response, fast output</done>
</task>

<task type="auto">
  <name>Task 3: Create multi-invocation tests</name>
  <files>
    tests/e2e/test_multi_invoke.py
  </files>
  <action>
Create `tests/e2e/test_multi_invoke.py` with tests for multi-iteration scenarios:

```python
"""Tests for multi-invocation scenarios: retry, failure memory, iteration chaining."""
import subprocess
import json
from fake_claude import fake_claude

def test_multi_invocation_different_responses():
    """Different responses for each invocation."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("First call response")
        .next_invocation()
        .respond_with_text("Second call response")
        .next_invocation()
        .respond_with_text("Third call response")
        .build()
    )

    # Call executable three times
    for i, expected in enumerate(["First", "Second", "Third"], 1):
        result = subprocess.run(
            [scenario.executable_path],
            capture_output=True,
            text=True
        )
        assert expected in result.stdout, f"Invocation {i}: expected '{expected}' in output"

    scenario.cleanup()

def test_invocation_count_tracking():
    """Scenario tracks how many times executable was called."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Response 1")
        .next_invocation()
        .respond_with_text("Response 2")
        .build()
    )

    # Call twice
    subprocess.run([scenario.executable_path], capture_output=True)
    subprocess.run([scenario.executable_path], capture_output=True)

    assert scenario.get_invocation_count() == 2
    scenario.cleanup()

def test_invocation_log_details():
    """Invocation log contains call details."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Test")
        .build()
    )

    subprocess.run([scenario.executable_path, "--arg1", "value"], capture_output=True)

    log = scenario.get_invocation_log()
    assert len(log) == 1
    assert "timestamp" in log[0]
    assert log[0]["invocation"] == 1
    scenario.cleanup()

def test_unconfigured_invocations_silent():
    """Invocations beyond configured ones return empty response."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Only first")
        .build()
    )

    # First call - configured
    r1 = subprocess.run([scenario.executable_path], capture_output=True, text=True)
    assert "Only first" in r1.stdout

    # Second call - unconfigured, should be silent pass-through
    r2 = subprocess.run([scenario.executable_path], capture_output=True, text=True)
    assert r2.returncode == 0
    # Should have minimal or no output (not "Only first" again)
    assert "Only first" not in r2.stdout

    scenario.cleanup()

def test_first_fails_second_succeeds(workspace, rslph_binary):
    """Simulate retry after failure: first invocation fails, second succeeds."""
    progress_initial = """# Tasks
- [ ] Task 1: Do something
"""
    progress_done = """# Tasks
- [x] Task 1: Do something
RALPH_DONE
"""

    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("Oops, I made an error")
        .with_exit_code(1)  # Failure
        .next_invocation()
        .respond_with_text("Fixed it!")
        .uses_write("PROGRESS.md", progress_done)
        .build()
    )

    workspace.with_claude_path(scenario.executable_path)
    workspace.write_progress(progress_initial)
    workspace.commit_all("Initial")

    # Run with 2 max iterations - should retry after first failure
    result = subprocess.run(
        [str(rslph_binary), "build", "--max-iterations", "2"],
        cwd=workspace.path,
        capture_output=True,
        text=True,
        timeout=60
    )

    # Verify both invocations happened
    assert scenario.get_invocation_count() >= 1

    scenario.cleanup()

def test_explicit_invocation_numbers():
    """Can configure specific invocation numbers (not just sequential)."""
    scenario = (
        fake_claude()
        .on_invocation(1)
        .respond_with_text("First")
        .on_invocation(5)  # Skip to 5
        .respond_with_text("Fifth")
        .build()
    )

    # Calls 1-4
    for _ in range(4):
        subprocess.run([scenario.executable_path], capture_output=True)

    # Call 5 should get "Fifth"
    r5 = subprocess.run([scenario.executable_path], capture_output=True, text=True)
    assert "Fifth" in r5.stdout

    scenario.cleanup()
```
  </action>
  <verify>
```bash
cd /Users/vmakaev/NonWork/rslph
python3 -m pytest tests/e2e/test_multi_invoke.py -v --tb=short
```
  </verify>
  <done>Multi-invocation tests pass: chained responses, count tracking, log details, retry scenarios</done>
</task>

</tasks>

<verification>
```bash
cd /Users/vmakaev/NonWork/rslph

# Run all edge case and multi-invoke tests
python3 -m pytest tests/e2e/test_edge_cases.py tests/e2e/test_multi_invoke.py -v

# Verify invocation tracking works
python3 -c "
from fake_claude import fake_claude
import subprocess

s = fake_claude().on_invocation(1).respond_with_text('test').build()
subprocess.run([s.executable_path], capture_output=True)
subprocess.run([s.executable_path], capture_output=True)
print(f'Invocation count: {s.get_invocation_count()}')
s.cleanup()
"
```
</verification>

<success_criteria>
1. Edge case methods work: crash_after, with_delay, send_raw, with_timeout_simulation
2. Invocation tracking: get_invocation_count(), get_invocation_log()
3. Multi-invocation: different responses per invocation number
4. Silent pass-through: unconfigured invocations return empty, exit 0
5. All tests in test_edge_cases.py and test_multi_invoke.py pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-e2e-testing-framework/07-03-SUMMARY.md`
</output>
