---
phase: 08-token-tracking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/build/tokens.rs
  - src/build/mod.rs
  - src/build/state.rs
  - src/build/iteration.rs
  - src/tui/event.rs
  - Cargo.toml
autonomous: true

must_haves:
  truths:
    - "Token usage is extracted from stream-json Usage events"
    - "Token usage accumulates correctly across multiple events in an iteration"
    - "Total tokens are updated in BuildContext for persistence"
  artifacts:
    - path: "src/build/tokens.rs"
      provides: "TokenUsage and IterationTokens structs with accumulation logic"
      exports: ["TokenUsage", "IterationTokens", "format_tokens"]
    - path: "src/tui/event.rs"
      provides: "SubprocessEvent::TokenUsage variant"
      contains: "TokenUsage"
  key_links:
    - from: "src/build/iteration.rs"
      to: "src/build/tokens.rs"
      via: "TokenUsage accumulation in parse_and_stream_line"
      pattern: "total_tokens\\.add_from_usage"
    - from: "src/build/iteration.rs"
      to: "src/tui/event.rs"
      via: "SubprocessEvent::TokenUsage emission"
      pattern: "SubprocessEvent::TokenUsage"
---

<objective>
Create core token tracking infrastructure: data types, accumulation logic, and event routing.

Purpose: Enable token consumption tracking as the foundation for display and persistence (TOK-01, TOK-02, TOK-03, TOK-04).

Output: TokenUsage struct, IterationTokens struct, SubprocessEvent::TokenUsage variant, format_tokens helper, integration with iteration loop.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-token-tracking/CONTEXT.md
@.planning/phases/08-token-tracking/RESEARCH.md
@src/subprocess/stream_json.rs
@src/build/state.rs
@src/build/iteration.rs
@src/tui/event.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create token types module with accumulation logic</name>
  <files>src/build/tokens.rs, src/build/mod.rs, Cargo.toml</files>
  <action>
Create new module `src/build/tokens.rs` with:

1. Add `human_format = "1.2"` to Cargo.toml dependencies

2. Define TokenUsage struct (cumulative totals):
```rust
#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct TokenUsage {
    pub input_tokens: u64,
    pub output_tokens: u64,
    pub cache_creation_input_tokens: u64,
    pub cache_read_input_tokens: u64,
}
```

3. Implement `add_from_usage` method on TokenUsage:
```rust
impl TokenUsage {
    pub fn add_from_usage(&mut self, usage: &crate::subprocess::Usage) {
        self.input_tokens += usage.input_tokens;
        self.output_tokens += usage.output_tokens;
        self.cache_creation_input_tokens += usage.cache_creation_input_tokens.unwrap_or(0);
        self.cache_read_input_tokens += usage.cache_read_input_tokens.unwrap_or(0);
    }
}
```

4. Define IterationTokens struct (per-iteration snapshot):
```rust
#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct IterationTokens {
    pub iteration: u32,
    pub input_tokens: u64,
    pub output_tokens: u64,
    pub cache_creation_input_tokens: u64,
    pub cache_read_input_tokens: u64,
}
```

5. Implement format_tokens helper using human_format:
```rust
use human_format::Formatter;

pub fn format_tokens(count: u64) -> String {
    if count == 0 {
        return "0".to_string();
    }
    Formatter::new()
        .with_decimals(1)
        .with_separator("")
        .format(count as f64)
}
```

6. Add `pub mod tokens;` to src/build/mod.rs and re-export types.
  </action>
  <verify>`cargo check` passes with no errors for the new module</verify>
  <done>TokenUsage, IterationTokens structs exist with accumulation and formatting methods</done>
</task>

<task type="auto">
  <name>Task 2: Add token fields to BuildContext and SubprocessEvent</name>
  <files>src/build/state.rs, src/tui/event.rs</files>
  <action>
1. In `src/build/state.rs`:
   - Add import: `use super::tokens::{TokenUsage, IterationTokens};`
   - Add fields to BuildContext struct:
     ```rust
     /// Per-iteration token usage history
     pub iteration_tokens: Vec<IterationTokens>,
     /// Cumulative token usage across all iterations
     pub total_tokens: TokenUsage,
     /// Current iteration's token usage (reset each iteration)
     pub current_iteration_tokens: TokenUsage,
     ```
   - Initialize these fields in `BuildContext::new` and `BuildContext::with_tui`:
     ```rust
     iteration_tokens: Vec::new(),
     total_tokens: TokenUsage::default(),
     current_iteration_tokens: TokenUsage::default(),
     ```

2. In `src/tui/event.rs`:
   - Add new variant to SubprocessEvent enum:
     ```rust
     /// Token usage update from stream event
     TokenUsage {
         input_tokens: u64,
         output_tokens: u64,
         cache_creation_input_tokens: u64,
         cache_read_input_tokens: u64,
     },
     ```
   - Add conversion in `impl From<SubprocessEvent> for AppEvent`:
     ```rust
     SubprocessEvent::TokenUsage { input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens } => {
         AppEvent::TokenUsage { input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens }
     }
     ```
   - Note: AppEvent::TokenUsage will be added in Plan 02 (TUI display)
  </action>
  <verify>`cargo check` passes (may have warnings about unused fields until Plan 02)</verify>
  <done>BuildContext has token fields; SubprocessEvent has TokenUsage variant</done>
</task>

<task type="auto">
  <name>Task 3: Integrate token accumulation into iteration loop</name>
  <files>src/build/iteration.rs</files>
  <action>
1. Add import at top: `use super::tokens::TokenUsage;`

2. Modify `parse_and_stream_line` to emit token usage events. Find the existing code block:
   ```rust
   if let Some(usage) = event.usage() {
       let ratio = (usage.input_tokens + usage.output_tokens) as f64 / 200_000.0;
       let _ = tui_tx.send(SubprocessEvent::Usage(ratio.min(1.0)));
   }
   ```
   Add BEFORE the ratio calculation:
   ```rust
   let _ = tui_tx.send(SubprocessEvent::TokenUsage {
       input_tokens: usage.input_tokens,
       output_tokens: usage.output_tokens,
       cache_creation_input_tokens: usage.cache_creation_input_tokens.unwrap_or(0),
       cache_read_input_tokens: usage.cache_read_input_tokens.unwrap_or(0),
   });
   ```

3. In `run_single_iteration`, after Step 6 (subprocess run) but before Step 7 (extract response):
   - Reset current_iteration_tokens at start of iteration (after Step 1):
     ```rust
     ctx.current_iteration_tokens = TokenUsage::default();
     ```

   - After parsing stream events (in both TUI and non-TUI paths), accumulate tokens:
     For TUI mode, add inside the while loop after `stream_response.process_event(&event)`:
     ```rust
     if event.is_assistant() {
         if let Some(usage) = event.usage() {
             ctx.current_iteration_tokens.add_from_usage(usage);
         }
     }
     ```
     Note: This requires passing `ctx` to the async block or restructuring.

     SIMPLER APPROACH: After the subprocess completes, use stream_response's accumulated tokens:
     After Step 7 (extract response text), add:
     ```rust
     // Accumulate tokens from this iteration
     let iteration_tokens = IterationTokens {
         iteration: ctx.current_iteration,
         input_tokens: stream_response.input_tokens,
         output_tokens: stream_response.output_tokens,
         cache_creation_input_tokens: 0, // StreamResponse doesn't track cache tokens yet
         cache_read_input_tokens: 0,
     };
     ctx.iteration_tokens.push(iteration_tokens);
     ctx.total_tokens.input_tokens += stream_response.input_tokens;
     ctx.total_tokens.output_tokens += stream_response.output_tokens;
     ```

4. IMPORTANT: Update StreamResponse in stream_json.rs to track cache tokens:
   - Add fields: `pub cache_creation_input_tokens: u64, pub cache_read_input_tokens: u64`
   - Update process_event to capture cache tokens from usage

5. Update the existing TRACE log to include all 4 token types:
   ```rust
   ctx.log(&format!(
       "[TRACE] Tokens: {} in / {} out / {} cache_write / {} cache_read",
       stream_response.input_tokens, stream_response.output_tokens,
       stream_response.cache_creation_input_tokens, stream_response.cache_read_input_tokens
   ));
   ```
  </action>
  <verify>`cargo check` passes; `cargo test` in build module passes</verify>
  <done>Token usage is accumulated per-iteration and stored in BuildContext</done>
</task>

</tasks>

<verification>
1. `cargo check` - All code compiles
2. `cargo test --lib` - All unit tests pass
3. Token types are properly exported from build module
</verification>

<success_criteria>
- TokenUsage and IterationTokens structs exist with serde derives
- format_tokens function formats numbers with SI suffixes (5200 -> "5.2k")
- BuildContext has iteration_tokens Vec and total_tokens fields
- SubprocessEvent::TokenUsage variant exists
- Iteration loop accumulates tokens from stream events into BuildContext
- StreamResponse tracks all 4 token types (input, output, cache_creation, cache_read)
</success_criteria>

<output>
After completion, create `.planning/phases/08-token-tracking/08-01-SUMMARY.md`
</output>
