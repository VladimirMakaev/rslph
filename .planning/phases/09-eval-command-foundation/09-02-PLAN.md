---
phase: 09-eval-command-foundation
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/eval/command.rs
  - src/main.rs
  - src/planning/command.rs
  - src/build/command.rs
autonomous: true

must_haves:
  truths:
    - "User can run rslph eval <path> and see plan+build execute"
    - "Eval creates isolated temp directory for execution"
    - "Eval reports execution time at completion"
    - "Eval reports token consumption at completion"
    - "Temp directory is cleaned up by default"
    - "With --keep flag, temp directory is preserved"
  artifacts:
    - path: "src/eval/command.rs"
      provides: "run_eval_command implementation"
      min_lines: 80
    - path: "src/main.rs"
      provides: "Commands::Eval dispatch"
      contains: "Commands::Eval"
  key_links:
    - from: "src/eval/command.rs"
      to: "run_plan_command"
      via: "function call"
      pattern: "run_plan_command"
    - from: "src/eval/command.rs"
      to: "run_build_command"
      via: "function call"
      pattern: "run_build_command"
    - from: "src/main.rs"
      to: "run_eval_command"
      via: "async call"
      pattern: "run_eval_command"
    - from: "src/eval/command.rs"
      to: "TokenUsage aggregation"
      via: "plan_tokens + build_tokens"
      pattern: "total_tokens.*plan_tokens.*build_tokens"
---

<objective>
Implement the eval command that orchestrates plan+build in an isolated temp directory and reports metrics.

Purpose: Enable users to run controlled evaluations with automatic isolation and metric collection (EVAL-01, EVAL-04, EVAL-05).
Output: Working `rslph eval <path>` command that creates temp workspace, runs plan+build, and reports time/tokens.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-eval-command-foundation/09-RESEARCH.md
@.planning/phases/09-eval-command-foundation/09-01-SUMMARY.md

Reference files:
@src/main.rs (command dispatch pattern)
@src/planning/command.rs (run_plan_command signature)
@src/build/command.rs (run_build_command signature)
@src/build/tokens.rs (format_tokens function, TokenUsage type)
@src/build/state.rs (BuildContext.total_tokens)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Modify run_plan_command to return TokenUsage</name>
  <files>src/planning/command.rs</files>
  <action>
Modify `run_plan_command` signature to return `(PathBuf, TokenUsage)` instead of just `PathBuf`.

1. Add import at top if not present:
```rust
use crate::build::tokens::TokenUsage;
```

2. Change function signature from:
```rust
pub async fn run_plan_command(...) -> color_eyre::Result<PathBuf>
```
to:
```rust
pub async fn run_plan_command(...) -> color_eyre::Result<(PathBuf, TokenUsage)>
```

3. In `run_basic_planning`, after collecting tokens from `stream_response`, create TokenUsage and return it:
```rust
// After parsing and writing progress file
let tokens = TokenUsage {
    input_tokens: stream_response.input_tokens,
    output_tokens: stream_response.output_tokens,
    cache_creation_input_tokens: stream_response.cache_creation_input_tokens,
    cache_read_input_tokens: stream_response.cache_read_input_tokens,
};

Ok((output_path, tokens))
```

4. Similarly update `run_adaptive_planning` to return `(PathBuf, TokenUsage)`.

5. Update the call to `run_adaptive_planning` in `run_plan_command` to propagate the tuple.
  </action>
  <verify>
  Run `cargo check` - should show errors in main.rs where run_plan_command is called (expected, will fix in Task 3).
  </verify>
  <done>
  - run_plan_command returns (PathBuf, TokenUsage)
  - run_basic_planning returns (PathBuf, TokenUsage)
  - run_adaptive_planning returns (PathBuf, TokenUsage)
  - TokenUsage is populated from stream_response fields
  </done>
</task>

<task type="auto">
  <name>Task 2: Modify run_build_command to return TokenUsage</name>
  <files>src/build/command.rs</files>
  <action>
Modify `run_build_command` to return `TokenUsage` instead of `()`.

1. Change function signature from:
```rust
pub async fn run_build_command(...) -> color_eyre::Result<()>
```
to:
```rust
pub async fn run_build_command(...) -> color_eyre::Result<TokenUsage>
```

2. In the main iteration loop, after `BuildState::Done`, return `ctx.total_tokens`:
```rust
BuildState::Done { reason } => {
    print_completion_message(&reason, &ctx);
    return Ok(ctx.total_tokens);
}
```

3. In `run_dry_run`, return `TokenUsage::default()` since no actual execution occurs:
```rust
fn run_dry_run(ctx: &BuildContext) -> color_eyre::Result<TokenUsage> {
    // ... existing dry run logic ...
    Ok(TokenUsage::default())
}
```

4. In `run_build_with_tui`, return tokens from the result:
```rust
async fn run_build_with_tui(...) -> color_eyre::Result<TokenUsage> {
    // ... in the loop, after BuildState::Done ...
    BuildState::Done { reason } => {
        // ... log completion ...
        break Ok(ctx.total_tokens);
    }
    BuildState::Failed { error } => {
        break Err(color_eyre::eyre::eyre!("Build failed: {}", error));
    }
}
```

5. Update the return type of `run_build_with_tui` and fix the final `result` variable to be `color_eyre::Result<TokenUsage>`.

Note: The TUI path with Failed state should still return Err, which is handled by the ? operator on the caller side.
  </action>
  <verify>
  Run `cargo check` - should show errors in main.rs where run_build_command is called (expected, will fix in Task 3).
  </verify>
  <done>
  - run_build_command returns TokenUsage
  - run_dry_run returns TokenUsage::default()
  - run_build_with_tui returns TokenUsage from ctx.total_tokens
  - Done state returns accumulated tokens
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement run_eval_command with token aggregation</name>
  <files>src/eval/command.rs</files>
  <action>
Replace the stub implementation in src/eval/command.rs with full implementation that aggregates tokens:

```rust
//! Eval command handler.
//!
//! Orchestrates plan+build execution in isolated temporary directories
//! for controlled benchmarking.

use std::path::PathBuf;
use std::time::{Duration, Instant};
use tempfile::TempDir;
use tokio_util::sync::CancellationToken;

use crate::build::run_build_command;
use crate::build::tokens::{format_tokens, TokenUsage};
use crate::config::Config;
use crate::planning::run_plan_command;
use crate::progress::ProgressFile;

use super::EvalResult;

/// Run the eval command (EVAL-01, EVAL-05).
///
/// Executes plan and build in an isolated temporary directory,
/// collecting metrics for tokens and timing.
///
/// # Arguments
///
/// * `project` - Path to project directory to evaluate
/// * `keep` - If true, preserve temp directory after completion
/// * `no_tui` - If true, disable TUI output
/// * `config` - Application configuration
/// * `cancel_token` - Token for graceful cancellation
///
/// # Returns
///
/// * `Ok(EvalResult)` - Eval completed with metrics
/// * `Err(e)` - Eval failed
pub async fn run_eval_command(
    project: String,
    keep: bool,
    no_tui: bool,
    config: &Config,
    cancel_token: CancellationToken,
) -> color_eyre::Result<EvalResult> {
    let start = Instant::now();

    // Step 1: Resolve project path
    let project_path = PathBuf::from(&project);
    if !project_path.exists() {
        return Err(color_eyre::eyre::eyre!(
            "Project path does not exist: {}",
            project_path.display()
        ));
    }

    // Step 2: Create isolated temp directory
    let workspace = TempDir::with_prefix(&format!("rslph-eval-{}-",
        project_path.file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("project")
    ))?;
    let working_dir = workspace.path().to_path_buf();

    println!("Eval workspace: {}", working_dir.display());

    // Step 3: Copy project files to temp directory
    copy_dir_recursive(&project_path, &working_dir)?;
    println!("Copied project files to workspace");

    // Step 4: Initialize git in workspace (required for VCS tracking)
    init_git_repo(&working_dir)?;

    // Step 5: Detect starting prompt
    let prompt = detect_eval_prompt(&working_dir)?;
    println!("Detected prompt: {} chars", prompt.len());

    // Step 6: Run plan command and capture tokens
    println!("\n=== PLANNING PHASE ===\n");
    let timeout = Duration::from_secs(config.max_iterations as u64 * 600);
    let (progress_path, plan_tokens) = run_plan_command(
        &prompt,
        false, // not adaptive
        config,
        &working_dir,
        cancel_token.clone(),
        timeout,
    )
    .await?;

    println!(
        "Planning tokens: In: {} | Out: {} | CacheW: {} | CacheR: {}",
        format_tokens(plan_tokens.input_tokens),
        format_tokens(plan_tokens.output_tokens),
        format_tokens(plan_tokens.cache_creation_input_tokens),
        format_tokens(plan_tokens.cache_read_input_tokens),
    );

    // Step 7: Run build command and capture tokens
    println!("\n=== BUILD PHASE ===\n");
    let build_tokens = run_build_command(
        progress_path.clone(),
        false, // not once
        false, // not dry-run
        no_tui || true, // force no-tui for eval to get clean output
        config,
        cancel_token.clone(),
    )
    .await?;

    println!(
        "Build tokens: In: {} | Out: {} | CacheW: {} | CacheR: {}",
        format_tokens(build_tokens.input_tokens),
        format_tokens(build_tokens.output_tokens),
        format_tokens(build_tokens.cache_creation_input_tokens),
        format_tokens(build_tokens.cache_read_input_tokens),
    );

    // Step 8: Aggregate tokens from plan + build
    let total_tokens = TokenUsage {
        input_tokens: plan_tokens.input_tokens + build_tokens.input_tokens,
        output_tokens: plan_tokens.output_tokens + build_tokens.output_tokens,
        cache_creation_input_tokens: plan_tokens.cache_creation_input_tokens + build_tokens.cache_creation_input_tokens,
        cache_read_input_tokens: plan_tokens.cache_read_input_tokens + build_tokens.cache_read_input_tokens,
    };

    // Step 9: Collect metrics from progress file
    let progress = ProgressFile::load(&progress_path)?;
    let iterations = progress.iteration_log.len() as u32;

    let elapsed_secs = start.elapsed().as_secs_f64();

    // Step 10: Handle workspace cleanup
    let workspace_path = if keep {
        let preserved = workspace.into_path();
        println!("\nWorkspace preserved at: {}", preserved.display());
        Some(preserved)
    } else {
        // TempDir will be dropped and cleaned up automatically
        drop(workspace);
        None
    };

    Ok(EvalResult {
        project,
        elapsed_secs,
        total_tokens,
        iterations,
        workspace_path,
    })
}

/// Copy directory contents recursively.
fn copy_dir_recursive(src: &PathBuf, dst: &PathBuf) -> std::io::Result<()> {
    if !dst.exists() {
        std::fs::create_dir_all(dst)?;
    }

    for entry in std::fs::read_dir(src)? {
        let entry = entry?;
        let src_path = entry.path();
        let dst_path = dst.join(entry.file_name());

        if src_path.is_dir() {
            // Skip .git directories
            if entry.file_name() == ".git" {
                continue;
            }
            copy_dir_recursive(&src_path, &dst_path)?;
        } else {
            std::fs::copy(&src_path, &dst_path)?;
        }
    }

    Ok(())
}

/// Initialize a git repository in the workspace.
fn init_git_repo(working_dir: &PathBuf) -> std::io::Result<()> {
    use std::process::Command;

    Command::new("git")
        .args(["init"])
        .current_dir(working_dir)
        .output()?;

    Command::new("git")
        .args(["config", "user.email", "eval@rslph.local"])
        .current_dir(working_dir)
        .output()?;

    Command::new("git")
        .args(["config", "user.name", "Eval"])
        .current_dir(working_dir)
        .output()?;

    // Initial commit so we have a clean baseline
    Command::new("git")
        .args(["add", "."])
        .current_dir(working_dir)
        .output()?;

    Command::new("git")
        .args(["commit", "-m", "Initial eval state", "--allow-empty"])
        .current_dir(working_dir)
        .output()?;

    Ok(())
}

/// Detect the eval prompt from the project directory.
///
/// Looks for prompt.txt or README.md in the project root.
fn detect_eval_prompt(working_dir: &PathBuf) -> color_eyre::Result<String> {
    // Priority 1: prompt.txt
    let prompt_file = working_dir.join("prompt.txt");
    if prompt_file.exists() {
        return Ok(std::fs::read_to_string(prompt_file)?);
    }

    // Priority 2: README.md
    let readme_file = working_dir.join("README.md");
    if readme_file.exists() {
        return Ok(std::fs::read_to_string(readme_file)?);
    }

    // Priority 3: PROMPT.md
    let prompt_md = working_dir.join("PROMPT.md");
    if prompt_md.exists() {
        return Ok(std::fs::read_to_string(prompt_md)?);
    }

    Err(color_eyre::eyre::eyre!(
        "No prompt file found. Expected prompt.txt, README.md, or PROMPT.md in project root"
    ))
}
```

Key implementation notes:
- Captures `plan_tokens` from `run_plan_command` return tuple
- Captures `build_tokens` from `run_build_command` return value
- Aggregates both into `total_tokens` for EvalResult
- Reports tokens at each phase and in final result
  </action>
  <verify>
  Run `cargo check` - should still show errors in main.rs (fixed in Task 4).
  </verify>
  <done>
  - run_eval_command aggregates tokens from plan + build
  - plan_tokens captured from run_plan_command return
  - build_tokens captured from run_build_command return
  - total_tokens = plan_tokens + build_tokens
  - Token summary printed at each phase
  </done>
</task>

<task type="auto">
  <name>Task 4: Update main.rs for new return types</name>
  <files>src/main.rs</files>
  <action>
Update main.rs to handle the new return types from run_plan_command and run_build_command.

1. Find the Commands::Plan match arm and update to destructure the tuple:
```rust
Commands::Plan { input, adaptive } => {
    // ... existing setup ...
    match run_plan_command(&prompt, adaptive, &config, &working_dir, cancel_token, timeout).await {
        Ok((output_path, _tokens)) => {
            // Tokens already printed by run_plan_command, just report path
            println!("Plan created: {}", output_path.display());
        }
        Err(e) => {
            eprintln!("Planning failed: {}", e);
            std::process::exit(1);
        }
    }
}
```

2. Find the Commands::Build match arm and update to handle TokenUsage return:
```rust
Commands::Build { progress, once, dry_run, no_tui } => {
    // ... existing setup ...
    match run_build_command(progress_path, once, dry_run, no_tui, &config, cancel_token).await {
        Ok(_tokens) => {
            // Tokens already printed by build command
            // Success exit
        }
        Err(e) => {
            eprintln!("Build failed: {}", e);
            std::process::exit(1);
        }
    }
}
```

3. Ensure Commands::Eval is wired (from prior plan step):
```rust
Commands::Eval { project, keep, no_tui } => {
    let cancel_token = setup_ctrl_c_handler();

    println!("Evaluating: {}", project);

    match run_eval_command(project, keep, no_tui, &config, cancel_token).await {
        Ok(result) => {
            println!("\n=== EVAL COMPLETE ===");
            println!("Project: {}", result.project);
            println!("Time: {:.1}s", result.elapsed_secs);
            println!("Iterations: {}", result.iterations);
            println!(
                "Tokens: In: {} | Out: {} | CacheW: {} | CacheR: {}",
                format_tokens(result.total_tokens.input_tokens),
                format_tokens(result.total_tokens.output_tokens),
                format_tokens(result.total_tokens.cache_creation_input_tokens),
                format_tokens(result.total_tokens.cache_read_input_tokens),
            );
            if let Some(path) = result.workspace_path {
                println!("Workspace: {}", path.display());
            }
        }
        Err(e) => {
            eprintln!("Eval failed: {}", e);
            std::process::exit(1);
        }
    }
}
```
  </action>
  <verify>
  Run `cargo build` and verify all commands compile. Test with:
  - `./target/debug/rslph plan --help`
  - `./target/debug/rslph build --help`
  - `./target/debug/rslph eval --help`
  </verify>
  <done>
  - Commands::Plan handles (PathBuf, TokenUsage) return
  - Commands::Build handles TokenUsage return
  - Commands::Eval dispatches to run_eval_command
  - All three commands compile and show help
  </done>
</task>

</tasks>

<verification>
1. `cargo build` succeeds
2. `./target/debug/rslph eval --help` shows project, --keep, --no-tui options
3. `cargo test` passes (existing tests may need minor updates for new signatures)
4. Create a test project with prompt.txt and run `rslph eval ./test-project` (will fail on Claude but verifies wiring)
</verification>

<success_criteria>
- run_plan_command returns (PathBuf, TokenUsage) with actual token data
- run_build_command returns TokenUsage with accumulated token data from BuildContext.total_tokens
- run_eval_command aggregates tokens from both phases into EvalResult.total_tokens
- main.rs dispatches all three commands correctly with new return types
- Timing metrics are reported at completion
- --keep flag preserves workspace directory
</success_criteria>

<output>
After completion, create `.planning/phases/09-eval-command-foundation/09-02-SUMMARY.md`
</output>
