---
phase: 10-eval-projects-and-testing
plan: 02
type: execute
wave: 2
depends_on: [10-01]
files_modified:
  - src/eval/mod.rs
  - src/eval/test_runner.rs
autonomous: true

must_haves:
  truths:
    - "Test cases can be loaded from JSONL content"
    - "Test runner executes program with stdin and captures stdout"
    - "Test results track passed/total counts"
    - "Output comparison handles whitespace trimming"
  artifacts:
    - path: "src/eval/test_runner.rs"
      provides: "Stdin/stdout test execution"
      exports: ["TestCase", "TestResult", "TestResults", "TestRunner"]
  key_links:
    - from: "src/eval/test_runner.rs"
      to: "std::process::Command"
      via: "spawn with stdin/stdout piping"
      pattern: "Command::new.*stdin.*stdout"
---

<objective>
Implement language-agnostic test runner for stdin/stdout testing

Purpose: Execute hidden tests against built programs using black-box input/output testing
Output: Test runner that can run any CLI program and compare outputs
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-eval-projects-and-testing/10-RESEARCH.md

@src/eval/mod.rs
@src/eval/projects.rs
@src/subprocess/runner.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test runner types and JSONL parser</name>
  <files>
    - src/eval/test_runner.rs
    - src/eval/mod.rs
  </files>
  <action>
1. Create src/eval/test_runner.rs with the following types:

```rust
use serde::Deserialize;
use std::path::PathBuf;
use std::time::Duration;

/// A single test case with input and expected output.
#[derive(Debug, Clone, Deserialize)]
pub struct TestCase {
    pub input: String,
    pub expected: String,
}

/// Result of running a single test case.
#[derive(Debug, Clone)]
pub struct TestResult {
    pub input: String,
    pub expected: String,
    pub actual: String,
    pub passed: bool,
}

/// Aggregated test results for all test cases.
#[derive(Debug, Clone)]
pub struct TestResults {
    /// Number of passing test cases
    pub passed: u32,
    /// Total number of test cases
    pub total: u32,
    /// Individual test case results
    pub cases: Vec<TestResult>,
}

impl TestResults {
    /// Calculate pass rate as a percentage (0.0 - 100.0).
    pub fn pass_rate(&self) -> f64 {
        if self.total == 0 {
            0.0
        } else {
            (self.passed as f64 / self.total as f64) * 100.0
        }
    }
}

/// Load test cases from JSONL content.
pub fn load_test_cases(content: &str) -> Vec<TestCase> {
    content
        .lines()
        .filter(|line| !line.trim().is_empty())
        .filter_map(|line| serde_json::from_str(line).ok())
        .collect()
}
```

2. Add unit tests for `load_test_cases`:
   - Test parsing valid JSONL
   - Test skipping empty lines
   - Test handling malformed lines gracefully

3. Update src/eval/mod.rs to add `mod test_runner;` and export types:
   `pub use test_runner::{TestCase, TestResult, TestResults, load_test_cases};`
  </action>
  <verify>
    cargo test test_runner -- --nocapture
  </verify>
  <done>
    - TestCase, TestResult, TestResults types defined
    - load_test_cases parses JSONL content
    - Unit tests for JSONL parsing pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement stdin/stdout test execution</name>
  <files>
    - src/eval/test_runner.rs
  </files>
  <action>
1. Add TestRunner struct to test_runner.rs:

```rust
use std::io::Write;
use std::process::{Command, Stdio};

/// Runner for executing stdin/stdout tests against a program.
pub struct TestRunner {
    /// Path to the program to test
    program_path: PathBuf,
    /// Timeout per test case
    timeout: Duration,
}

impl TestRunner {
    /// Create a new test runner for the given program.
    pub fn new(program_path: PathBuf) -> Self {
        Self {
            program_path,
            timeout: Duration::from_secs(5),
        }
    }

    /// Set the timeout per test case.
    pub fn with_timeout(mut self, timeout: Duration) -> Self {
        self.timeout = timeout;
        self
    }

    /// Run all test cases and collect results.
    pub fn run_tests(&self, tests: &[TestCase]) -> TestResults {
        let mut passed = 0;
        let mut cases = Vec::new();

        for test in tests {
            let result = self.run_single_test(test);
            if result.passed {
                passed += 1;
            }
            cases.push(result);
        }

        TestResults {
            passed,
            total: tests.len() as u32,
            cases,
        }
    }

    fn run_single_test(&self, test: &TestCase) -> TestResult {
        // Spawn process with stdin/stdout piping
        let child_result = Command::new(&self.program_path)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn();

        let mut child = match child_result {
            Ok(c) => c,
            Err(e) => {
                return TestResult {
                    input: test.input.clone(),
                    expected: test.expected.clone(),
                    actual: format!("spawn error: {}", e),
                    passed: false,
                };
            }
        };

        // Write input to stdin
        if let Some(mut stdin) = child.stdin.take() {
            let _ = stdin.write_all(test.input.as_bytes());
            let _ = stdin.write_all(b"\n");
        }

        // Wait for output (with implicit timeout via wait_with_output)
        match child.wait_with_output() {
            Ok(output) => {
                let actual = String::from_utf8_lossy(&output.stdout)
                    .trim()
                    .to_string();
                let expected = test.expected.trim();
                TestResult {
                    input: test.input.clone(),
                    expected: expected.to_string(),
                    actual: actual.clone(),
                    passed: actual == expected,
                }
            }
            Err(e) => TestResult {
                input: test.input.clone(),
                expected: test.expected.clone(),
                actual: format!("execution error: {}", e),
                passed: false,
            },
        }
    }
}
```

2. Add unit/integration tests for TestRunner:
   - Test with /bin/echo (simple stdout)
   - Test with a failing case (output mismatch)
   - Test with program that reads stdin (cat)
   - Test pass_rate calculation

Note: Use synchronous std::process::Command (not tokio) since test execution is after build completes and doesn't need async streaming.
  </action>
  <verify>
    cargo test test_runner -- --nocapture
  </verify>
  <done>
    - TestRunner struct with new, with_timeout, run_tests, run_single_test
    - Spawns process with stdin/stdout piping
    - Trims whitespace from both expected and actual output
    - Handles spawn and execution errors gracefully
    - Unit tests pass including echo/cat verification
  </done>
</task>

</tasks>

<verification>
- `cargo build` succeeds
- `cargo test test_runner` passes all tests
- Test runner can execute simple programs (/bin/echo, /bin/cat) and compare output
- JSONL parsing handles edge cases (empty lines, malformed JSON)
</verification>

<success_criteria>
- TestCase, TestResult, TestResults types implemented (PROJ-02 partial)
- JSONL parsing for test data (PROJ-03 partial)
- Stdin/stdout test execution working
- Pass rate calculation correct
</success_criteria>

<output>
After completion, create `.planning/phases/10-eval-projects-and-testing/10-02-SUMMARY.md`
</output>
