---
phase: 12-multi-trial-results
plan: 03
type: execute
wave: 2
depends_on: ["12-02"]
files_modified:
  - src/eval/command.rs
autonomous: true

must_haves:
  truths:
    - "Multi-trial results saved to timestamped JSON file in eval_dir"
    - "JSON contains all trial summaries with metrics"
    - "JSON contains aggregated statistics"
    - "File path displayed after save"
  artifacts:
    - path: "src/eval/command.rs"
      provides: "Multi-trial JSON serialization"
      contains: "save_multi_trial_result"
  key_links:
    - from: "src/eval/command.rs"
      to: "serde_json"
      via: "to_string_pretty"
      pattern: "serde_json::to_string_pretty"
---

<objective>
Add JSON serialization for multi-trial results (EVAL-08)

Purpose: Store multi-trial results in a structured JSON file for later comparison and analysis
Output: Timestamped JSON file with trial summaries and statistics saved to config.eval_dir
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-multi-trial-results/12-RESEARCH.md

# Prior plan artifacts
@.planning/phases/12-multi-trial-results/12-02-SUMMARY.md

# Current implementation
@src/eval/command.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create serializable types for multi-trial JSON output</name>
  <files>src/eval/command.rs</files>
  <action>
Add new serializable structs for the multi-trial JSON format. Place near existing SerializableResult:

```rust
#[derive(Debug, Serialize, Deserialize)]
struct SerializableMultiTrialResult {
    project: String,
    timestamp: String,
    trial_count: u32,
    trials: Vec<SerializableTrialSummary>,
    statistics: SerializableStatistics,
}

#[derive(Debug, Serialize, Deserialize)]
struct SerializableTrialSummary {
    trial_num: u32,
    elapsed_secs: f64,
    iterations: u32,
    tokens: SerializableTokens,
    test_results: Option<SerializableTestResults>,
    workspace_path: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct SerializableStatistics {
    pass_rate: SerializableStatSummary,
    elapsed_secs: SerializableStatSummary,
    total_input_tokens: SerializableStatSummary,
    total_output_tokens: SerializableStatSummary,
    iterations: SerializableStatSummary,
}

#[derive(Debug, Serialize, Deserialize)]
struct SerializableStatSummary {
    mean: f64,
    variance: f64,
    std_dev: f64,
    min: f64,
    max: f64,
    count: usize,
}
```

Note: Add Deserialize derive to enable loading for compare command later.
  </action>
  <verify>`cargo check` passes</verify>
  <done>Serializable types defined with Serialize and Deserialize derives</done>
</task>

<task type="auto">
  <name>Task 2: Implement save_multi_trial_result and integrate with run_eval_command</name>
  <files>src/eval/command.rs</files>
  <action>
1. Add function `save_multi_trial_result`:
   ```rust
   fn save_multi_trial_result(
       eval_dir: &Path,
       result: &MultiTrialResult,
   ) -> color_eyre::Result<PathBuf> {
       // Generate filename: eval-results-{project}-{YYYY-MM-DD}.json
       let filename = format!(
           "eval-results-{}-{}.json",
           result.project,
           Utc::now().format("%Y-%m-%d")
       );
       let path = eval_dir.join(&filename);

       // Convert to serializable format
       let serializable = convert_to_serializable(result);

       // Write pretty-printed JSON
       let json = serde_json::to_string_pretty(&serializable)?;
       std::fs::write(&path, json)?;

       Ok(path)
   }
   ```

2. Add helper function `convert_to_serializable(result: &MultiTrialResult) -> SerializableMultiTrialResult`:
   - Convert each EvalResult to SerializableTrialSummary
   - Convert TrialStatistics to SerializableStatistics
   - Include std_dev() in the serialized output

3. In `run_eval_command`, after computing statistics:
   - Call `save_multi_trial_result(&config.eval_dir, &multi_result)?`
   - Print: `Results saved to: {path}`

4. Ensure individual trial result.json files are still saved in each workspace (keep existing save_result_json calls)

5. Add unit test `test_save_multi_trial_result`:
   - Create a mock MultiTrialResult with 2 trials
   - Save to temp directory
   - Verify JSON file exists and contains expected fields
  </action>
  <verify>`cargo test --lib test_save_multi_trial` passes, `cargo check` passes</verify>
  <done>Multi-trial results saved to eval_dir/eval-results-{project}-{date}.json</done>
</task>

</tasks>

<verification>
- [ ] `cargo test --lib` - all tests pass
- [ ] `cargo check` - no compile errors
- [ ] JSON file contains: project, timestamp, trial_count, trials array, statistics object
- [ ] Each trial in JSON has: trial_num, elapsed_secs, iterations, tokens, test_results, workspace_path
</verification>

<success_criteria>
1. After `rslph eval calculator --trials 3`, file `eval-results-calculator-2026-01-21.json` exists in eval_dir
2. JSON file is pretty-printed and human-readable
3. JSON contains all 3 trial summaries with their metrics
4. JSON contains statistics with mean, variance, std_dev, min, max for each metric
5. Individual workspace result.json files still saved (backward compatible)
</success_criteria>

<output>
After completion, create `.planning/phases/12-multi-trial-results/12-03-SUMMARY.md`
</output>
