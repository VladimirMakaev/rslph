---
phase: 12-multi-trial-results
plan: 05
type: execute
wave: 5
depends_on: ["12-04"]
files_modified:
  - tests/e2e/test_eval_integration.rs
autonomous: true

must_haves:
  truths:
    - "E2E test verifies --trials flag is accepted"
    - "E2E test verifies compare command with valid files"
    - "E2E test verifies compare command error handling"
  artifacts:
    - path: "tests/e2e/test_eval_integration.rs"
      provides: "E2E tests for multi-trial and compare"
      contains: "test_eval_trials_flag"
  key_links:
    - from: "tests/e2e/test_eval_integration.rs"
      to: "rslph binary"
      via: "Command::cargo_bin"
      pattern: "cargo_bin.*rslph"
---

<objective>
Add E2E tests for multi-trial and compare features

Purpose: Verify CLI integration and error handling for new features
Output: E2E tests covering --trials flag parsing, compare command usage, and error cases
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-multi-trial-results/12-RESEARCH.md

# Prior plan artifacts
@.planning/phases/12-multi-trial-results/12-04-SUMMARY.md

# Current E2E tests
@tests/e2e/test_eval_integration.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add E2E tests for --trials flag</name>
  <files>tests/e2e/test_eval_integration.rs</files>
  <action>
Add E2E tests for the --trials flag:

1. `test_eval_trials_flag_help`:
   - Run `rslph eval --help`
   - Assert output contains "--trials"
   - Assert output contains "Number of" (from doc comment)

2. `test_eval_trials_invalid_value`:
   - Run `rslph eval calculator --trials abc`
   - Assert exit code is non-zero
   - Assert stderr contains "invalid value" or similar

3. `test_eval_trials_zero`:
   - Run `rslph eval calculator --trials 0`
   - Assert appropriate behavior (could be error or run 0 trials)
   - Document expected behavior

Note: Full multi-trial execution test would require fake Claude, which is too heavyweight for this plan. Focus on CLI parsing and validation.
  </action>
  <verify>`cargo test --test test_eval_integration` passes</verify>
  <done>E2E tests verify --trials flag parsing and validation</done>
</task>

<task type="auto">
  <name>Task 2: Add E2E tests for compare command</name>
  <files>tests/e2e/test_eval_integration.rs</files>
  <action>
Add E2E tests for the compare command:

1. `test_compare_help`:
   - Run `rslph compare --help`
   - Assert output contains "file1" and "file2"
   - Assert output contains "Compare" (from doc comment)

2. `test_compare_missing_file`:
   - Run `rslph compare /nonexistent/file1.json /nonexistent/file2.json`
   - Assert exit code is non-zero
   - Assert stderr contains "Failed to read" or file path

3. `test_compare_missing_args`:
   - Run `rslph compare`
   - Assert exit code is non-zero
   - Assert stderr indicates missing required arguments

4. `test_compare_valid_files`:
   - Create two temporary JSON files with valid multi-trial result format
   - Run `rslph compare temp1.json temp2.json`
   - Assert exit code is 0
   - Assert stdout contains "Comparing results"
   - Assert stdout contains "Pass Rate"
   - Cleanup temp files

For the valid files test, create minimal valid JSON:
```json
{
  "project": "test",
  "timestamp": "2026-01-21",
  "trial_count": 1,
  "trials": [{
    "trial_num": 1,
    "elapsed_secs": 10.0,
    "iterations": 1,
    "tokens": {"input": 100, "output": 50, "cache_creation": 0, "cache_read": 0},
    "test_results": {"passed": 5, "total": 10, "pass_rate": 50.0},
    "workspace_path": "/tmp/test"
  }],
  "statistics": {
    "pass_rate": {"mean": 50.0, "variance": 0.0, "std_dev": 0.0, "min": 50.0, "max": 50.0, "count": 1},
    "elapsed_secs": {"mean": 10.0, "variance": 0.0, "std_dev": 0.0, "min": 10.0, "max": 10.0, "count": 1},
    "total_input_tokens": {"mean": 100.0, "variance": 0.0, "std_dev": 0.0, "min": 100.0, "max": 100.0, "count": 1},
    "total_output_tokens": {"mean": 50.0, "variance": 0.0, "std_dev": 0.0, "min": 50.0, "max": 50.0, "count": 1},
    "iterations": {"mean": 1.0, "variance": 0.0, "std_dev": 0.0, "min": 1.0, "max": 1.0, "count": 1}
  }
}
```
  </action>
  <verify>`cargo test --test test_eval_integration` passes</verify>
  <done>E2E tests verify compare command with valid and invalid inputs</done>
</task>

</tasks>

<verification>
- [ ] `cargo test --test test_eval_integration` - all tests pass
- [ ] Tests cover: --trials flag, compare command, error cases
- [ ] No flaky tests (all assertions are deterministic)
</verification>

<success_criteria>
1. `test_eval_trials_flag_help` verifies --trials appears in help
2. `test_compare_valid_files` successfully compares two JSON files
3. `test_compare_missing_file` shows appropriate error message
4. All E2E tests pass: `cargo test --test test_eval_integration`
</success_criteria>

<output>
After completion, create `.planning/phases/12-multi-trial-results/12-05-SUMMARY.md`
</output>
